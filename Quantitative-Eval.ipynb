{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QiyuanTan/AI-study-guide-backend/blob/master/Quantitative-Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "This notebook implemented an AI-agent workflow that allows to evaluate anything with a group of customized personas and criteria\n",
        "\n",
        "The workflow goes as follows:  \n",
        "1. Prompt set up\n",
        "  1. Set evaluation subject and items\n",
        "  2. Create persona definitions and criteria\n",
        "  3. Set up the prompt template for personas\n",
        "2. Generate evaluations\n",
        "  1. For each persona, apply the prompt template and make an API call\n",
        "  2. Parse and store the evaluations\n",
        "3. Process evaluations\n",
        "  1. Normalize the results\n",
        "  2. Apply weights\n",
        "4. Display the results"
      ],
      "metadata": {
        "id": "iP-QfH2Pd-JI"
      },
      "id": "iP-QfH2Pd-JI"
    },
    {
      "metadata": {
        "id": "3012def33677d63c"
      },
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "## Get a Google API key\n",
        "> You could skip this step if you already have one in Google AI Studio\n",
        "\n",
        "1. Select the `Secrets` ![secrets.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABsAAAAYCAYAAAALQIb7AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAHeSURBVEhL7ZS/TttQFMY/V1bJkCVDlKhhpO+Au1FFCYIJSChVEgYKCX+GNpHH/n0BWOgL0IWKgIrKM/h6zpitLIkcFA+OlFDZ+jo5so3bWigw5Sfdwef63u985x4diSTxSDwJBh6SqdhEmIpNhKnYRJDCxtXt71v8vLrCj8tLGIYBAMg8y2B9vYiXCwuYeToTPBKJO2LdbhfvP37Ar+trb3jM87k5fPn0GclkMrj1f+jBsiw2VJXZfI7V3RpbrRYdx6HjOGy326zu1pjN59hQVVqW5T0aCZ9Y8/yc2XyOb+vvQi/r9/vcO9hnNp+LvEqVMns3PZLkuEFGoxE0ISDLMjZebSAej0PoAqvFAlaLBQhdIJFIYLOyCVmW/eWJipt176bHUqXM1+USDcOgaZrc2tkeZ7i1s03TNL1G/4l7X6izR8HNZDgcsqGqXFxeotB1kqQmNK4U1rhSWKMmNJKk0HUuLi/99W1cR8G4z1ksFsMLRYFt2zj9forBYABlXsHFWRMXZ00o8wpM08TJtxPYtu3POCquM96z9YNvE/z24hMjyU6nwzeexgiuvYN9GoYx/j9YtrDyudxpkHQ6ja/Hx6hVq0ilUpAkCZIkYTYzi0a9jqPDw/tNj7Bx9ZD8AfYRRClKY9QBAAAAAElFTkSuQmCC) tab on left\n",
        "2. Go to `Gemini API keys` -> `Manage API keys in Google AI Studio`\n",
        "\n"
      ],
      "id": "3012def33677d63c"
    },
    {
      "metadata": {
        "id": "ec89d890b9154553"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "id": "ec89d890b9154553"
    },
    {
      "metadata": {
        "id": "4bf7d69643165104"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/requirements.txt"
      ],
      "id": "4bf7d69643165104",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-07-03T01:17:02.790398Z",
          "start_time": "2025-07-03T01:16:59.798202Z"
        },
        "id": "54c3d82acd1ccca7"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import operator\n",
        "import re\n",
        "from copy import deepcopy\n",
        "from typing import TypedDict, Annotated, List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.constants import END, START\n",
        "from langgraph.graph import StateGraph\n",
        "from tqdm import tqdm\n",
        "from tenacity import retry, stop_after_attempt\n",
        "\n",
        "# Load environment variables (GOOGLE_API_KEY should be set either in .env file or in the secrets)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except KeyError:\n",
        "    raise KeyError(\"Please set the GOOGLE_API_KEY in your secrets.\")\n",
        "\n",
        "# LLM config\n",
        "# At here you can change the model, tweak its parameters, or even use different LLM provider\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)"
      ],
      "id": "54c3d82acd1ccca7",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ed1c6cec5d5282d3"
      },
      "cell_type": "markdown",
      "source": [
        "# Prompt setup"
      ],
      "id": "ed1c6cec5d5282d3"
    },
    {
      "metadata": {
        "id": "a7abbc9756ca4ba2"
      },
      "cell_type": "markdown",
      "source": [
        "First setup evaluation subject and items to evaluate"
      ],
      "id": "a7abbc9756ca4ba2"
    },
    {
      "metadata": {
        "id": "e60a304581875d42"
      },
      "cell_type": "code",
      "source": [
        "# What to evaluate\n",
        "EVALUATION_SUBJECT = \"band\"\n",
        "\n",
        "# List of items to evaluate\n",
        "ITEMS = [\n",
        "    \"The Beatles\", \"Led Zeppelin\", \"Pink Floyd\", \"Queen\", \"The Rolling Stones\",\n",
        "    \"Metallica\", \"Megadeth\", \"Black Sabbath\", \"Iron Maiden\", \"Tool\"\n",
        "]"
      ],
      "id": "e60a304581875d42",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bdded21db99935c5"
      },
      "cell_type": "markdown",
      "source": [
        "Use One of the following two cells to initialize the personas and the evaluation criteria."
      ],
      "id": "bdded21db99935c5"
    },
    {
      "metadata": {
        "id": "c923589bfb3eec25"
      },
      "cell_type": "code",
      "source": [
        "# Initialize personas and criteria by a llm\n",
        "persona_nums = 5\n",
        "criterion_nums = 5\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are setting up an experiment that asks diverse group of virtual individuals to evaluate {len(ITEMS)} {EVALUATION_SUBJECT}s. You now need to set define these personas and the criteria for them to follow\n",
        "\n",
        "The definition of personas should follow the following principle:\n",
        "- The group should have a great diversity so that they could reflect the diverse opinions on {EVALUATION_SUBJECT}s.\n",
        "- Do not give simular/repeated persona definitions.\n",
        "- You are speaking to the personas when you give their definitions. Your persona definitions should start with \"You're\".\n",
        "\n",
        "The definition of the criteria should follow the following principle:\n",
        "- Be clear and specific\n",
        "- Each criterion should measure a unique aspect\n",
        "- The personas will fill in a number from 1-5 for each criterion, so make sure each it can be answered with a number\n",
        "- The higher the score, the more that {EVALUATION_SUBJECT} is preferred.\n",
        "\n",
        "You are asked to give {persona_nums} persona definitions and {criterion_nums} criteria\n",
        "\n",
        "Your response should follow the following JSON format\n",
        "```json\n",
        "{{\n",
        "    \"personas\": {{\n",
        "        \"persona_name_here\": \"persona's word view here\",\n",
        "        // ...More personas\n",
        "    }},\n",
        "    \"criteria\": {{\n",
        "        \"criterion_name_here\": \"criterion definition here\",\n",
        "        // ... More criteria\n",
        "    }},\n",
        "    \"persona_role\": \"Describe the personas' identity in short words, eg. critics. You are not supposed to say they are virtual\",\n",
        "    \"instruction\": \"here you give a base instruction for the persona s, eg. you were asked to evaluate {EVALUATION_SUBJECT}s\"\n",
        "}}\n",
        "```\n",
        "\n",
        "Do not include any commentary outside the JSON block.\n",
        "\"\"\"\n",
        "\n",
        "def clean_json_string(text: str) -> str:\n",
        "    cleaned = re.sub(r\"```(?:json)?\", \"\", text)\n",
        "    return cleaned.replace(\"```\", \"\").strip()\n",
        "\n",
        "@retry(stop=stop_after_attempt(3))\n",
        "def generate_persona_criteria():\n",
        "    response = llm.invoke(prompt)\n",
        "    response_cleaned = clean_json_string(response.content)\n",
        "    data = json.loads(response_cleaned)\n",
        "    return data[\"personas\"], data['criteria'], data['persona_role'], data['instruction']\n",
        "\n",
        "PERSONAS, CRITERIA, PERSONA_ROLE, INSTRUCTION = generate_persona_criteria()\n",
        "print(PERSONAS)\n",
        "print(CRITERIA)\n",
        "print(PERSONA_ROLE)\n",
        "print(INSTRUCTION)"
      ],
      "id": "c923589bfb3eec25",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a23f165a748c41f3"
      },
      "cell_type": "code",
      "source": [
        "# Initialize personas and criteria manually\n",
        "\n",
        "# Persona definitions\n",
        "# Each persona has generates a call to the LLM, so please be aware of the rate limits of your LLM provider\n",
        "# For Gemini, the rate limit is 10 calls per minute for 2.5 flash models and 5 calls per minute for 2.5 pro models.\n",
        "PERSONAS = {\n",
        "    \"metalhead\": \"You're in your 30s, a lifelong metal fan. You value power, aggression, instrumental mastery, and complexity. You dismiss pop and overproduced music as shallow.\",\n",
        "    \"popstar\": \"You're in your 20s, immersed in social media culture. You love global accessibility, emotional resonance, and catchy choruses. You believe great bands bring joy and unity.\",\n",
        "    \"boomer\": \"You're in your 70s. You grew up during the golden age of rock and believe greatness is rooted in legacy, songwriting, and timeless appeal. Newer music feels synthetic to you.\",\n",
        "    \"genz\": \"You're a teenager, online-native, and value diversity, identity, and innovation in music. You're drawn to bands that say something real or break genre rules.\",\n",
        "    \"indie\": \"You're in your 30s, an art-school type who craves authenticity, emotion, and underground cool. You dislike commercial polish and love expressive weirdness.\",\n",
        "}\n",
        "\n",
        "# Criteria for evaluation\n",
        "CRITERIA = {\n",
        "    \"Musical Innovation\": \"Pioneering ideas, new sounds, genre blending.\",\n",
        "    \"Cultural Impact\": \"Broader societal influence, pop culture penetration.\",\n",
        "    \"Lyrical or Thematic Depth\": \"Narrative richness, philosophical weight, relatability.\",\n",
        "    \"Technical Proficiency\": \"Musical complexity, virtuosity, performance execution.\",\n",
        "    \"Live Performance Strength\": \"Energy, presence, crowd connection on stage.\",\n",
        "    \"Legacy & Longevity\": \"Enduring influence across generations and artists.\"\n",
        "}\n",
        "\n",
        "# Instructions/background information for the personas\n",
        "PERSONA_ROLE = \"music critic\"\n",
        "INSTRUCTION = f\"You have been asked to evaluate the greatness of {len(ITEMS)} historically significant bands across genres including rock, metal, pop, and progressive.\""
      ],
      "id": "a23f165a748c41f3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3db76147acfc063a"
      },
      "cell_type": "markdown",
      "source": [
        "Establish the template and test the prompt"
      ],
      "id": "3db76147acfc063a"
    },
    {
      "metadata": {
        "id": "6146343afd80a017"
      },
      "cell_type": "code",
      "source": [
        "# Prompt template\n",
        "def make_prompt(persona_description):\n",
        "    return f\"\"\"You are a {PERSONA_ROLE} with the following worldview:\n",
        "\n",
        "{persona_description}\n",
        "\n",
        "{INSTRUCTION}\n",
        "\n",
        "Evaluate each {EVALUATION_SUBJECT} based on the following {len(CRITERIA)} criteria, scoring from 1 (low) to 5 (high):\n",
        "\n",
        "{\"\".join(f\"{key}: {value}{chr(10)}\" for key, value in CRITERIA.items())}\n",
        "Here are the {EVALUATION_SUBJECT}s to evaluate:\n",
        "{chr(10).join('- ' + item for item in ITEMS)}\n",
        "\n",
        "Please respond ONLY in the following strict JSON format:\n",
        "\n",
        "```json\n",
        "{{\n",
        "  \"ratings\": [\n",
        "    {{\n",
        "      \"item\": \"the corresponding {EVALUATION_SUBJECT} name here, following the ordering in the given list\"{\"\".join(f',{chr(10)}      \"{criteria}\": int' for criteria in CRITERIA)}\n",
        "    }},\n",
        "    // ...More {EVALUATION_SUBJECT} evaluations here\n",
        "  ],\n",
        "  \"justification\": \"Your paragraph explaining the ratings here.\",\n",
        "  \"ranking\": [\"{EVALUATION_SUBJECT}1\", \"{EVALUATION_SUBJECT}2\", ..., \"{EVALUATION_SUBJECT}{len(ITEMS)}\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "- The ratings list must include all {len(ITEMS)} {EVALUATION_SUBJECT}s.\n",
        "- The ranking list must be in your personal order (1st to {len(ITEMS)}th).\n",
        "- Do not include any commentary outside the JSON block.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example prompt:\")\n",
        "print(make_prompt(list(PERSONAS.values())[0]))"
      ],
      "id": "6146343afd80a017",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "12b4a0d5df2e8209"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate Evaluations"
      ],
      "id": "12b4a0d5df2e8209"
    },
    {
      "cell_type": "code",
      "id": "2696d0acd5ea46f3",
      "metadata": {
        "id": "2696d0acd5ea46f3"
      },
      "source": [
        "print(\"Starting evaluation...\")\n",
        "\n",
        "# Clean LLM output\n",
        "def clean_json_string(text: str) -> str:\n",
        "    cleaned = re.sub(r\"```(?:json)?\", \"\", text)\n",
        "    return cleaned.replace(\"```\", \"\").strip()\n",
        "\n",
        "# Parse JSON\n",
        "def parse_json_response(response):\n",
        "    response_cleaned = clean_json_string(response)\n",
        "    data = json.loads(response_cleaned)\n",
        "    ratings = data[\"ratings\"]\n",
        "    justification = data[\"justification\"]\n",
        "    ranking = data[\"ranking\"]\n",
        "\n",
        "    ranking_column = []\n",
        "    for i, item in enumerate(ranking):\n",
        "        ranking_column += [{\"item\": item, \"rank\": i + 1}]\n",
        "\n",
        "    df = pd.DataFrame(ratings)\n",
        "\n",
        "    df = pd.merge(df, pd.DataFrame(ranking_column), on=\"item\", how=\"left\")\n",
        "    df.columns = [EVALUATION_SUBJECT] + list(CRITERIA.keys()) + [\"Rank\"]\n",
        "    return df, justification\n",
        "\n",
        "# Get llm response\n",
        "@retry(stop=stop_after_attempt(3))\n",
        "async def get_llm_response(prompt):\n",
        "    response = await llm.ainvoke(prompt)\n",
        "    return parse_json_response(response.content)\n",
        "\n",
        "# State definition\n",
        "class Vote(TypedDict):\n",
        "    df: pd.DataFrame\n",
        "    justification: str\n",
        "    persona: str\n",
        "\n",
        "class State(TypedDict):\n",
        "    votes: Annotated[List[Vote], operator.add]\n",
        "\n",
        "# Initialize progress bar\n",
        "try:\n",
        "    pbar.close()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "pbar = tqdm(f\"Evaluating {EVALUATION_SUBJECT}s with personas\", total=len(PERSONAS), unit=\"persona\")\n",
        "\n",
        "# Agent node\n",
        "def make_agent_node(persona_key):\n",
        "    async def node(state):\n",
        "        persona = PERSONAS[persona_key]\n",
        "        prompt = make_prompt(persona)\n",
        "        df, justification = await get_llm_response(prompt)\n",
        "\n",
        "        state['votes'] = [{\n",
        "                \"df\": df,\n",
        "                \"justification\": justification,\n",
        "                \"persona\": persona_key,\n",
        "        }]\n",
        "\n",
        "        pbar.update(1)\n",
        "        return state\n",
        "    return node\n",
        "\n",
        "# Graph build\n",
        "agent_keys = list(PERSONAS.keys())\n",
        "\n",
        "graph = StateGraph(State)\n",
        "for agent in agent_keys:\n",
        "    graph.add_node(agent, make_agent_node(agent))\n",
        "\n",
        "# Graph edges\n",
        "for agent in agent_keys:\n",
        "    graph.add_edge(START, agent)\n",
        "graph.add_edge([agent for agent in agent_keys], END)\n",
        "\n",
        "# Run\n",
        "compiled = graph.compile()\n",
        "results = await compiled.ainvoke({\n",
        "    \"votes\": [],\n",
        "})\n",
        "\n",
        "votes = results['votes']\n",
        "criteria_keys = list(CRITERIA.keys())\n",
        "\n",
        "# Display results\n",
        "for vote in votes:\n",
        "    print(f\"Evaluation by persona: {vote['persona']}\")\n",
        "    display(vote['df'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6d2c349e713329ab"
      },
      "cell_type": "markdown",
      "source": [
        "# Process Evaluations"
      ],
      "id": "6d2c349e713329ab"
    },
    {
      "metadata": {
        "id": "bcb25eb75940937f"
      },
      "cell_type": "markdown",
      "source": [
        "## Optional: Normalize the scores\n",
        "This step reduces the bias from individual scoring tendencies."
      ],
      "id": "bcb25eb75940937f"
    },
    {
      "cell_type": "code",
      "id": "dac7354e0e335825",
      "metadata": {
        "id": "dac7354e0e335825"
      },
      "source": [
        "# Normalize the scores\n",
        "for vote in votes:\n",
        "    all_values = vote['df'][criteria_keys].values.flatten()\n",
        "    mean = all_values.mean()\n",
        "    std_dev = all_values.std()\n",
        "    vote['df'][criteria_keys] = (vote['df'][criteria_keys] - mean) / std_dev\n",
        "\n",
        "# Ensure all scores are non-negative\n",
        "min_z = min([vote['df'][criteria_keys].min().min() for vote in votes])\n",
        "for vote in votes:\n",
        "    vote['df'][criteria_keys] = vote['df'][criteria_keys] - min_z\n",
        "\n",
        "# Display normalized scores\n",
        "for vote in votes:\n",
        "    print(f\"Normalized scores for persona: {vote['persona']}\")\n",
        "    display(vote['df'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a91fa6d97f38eef9"
      },
      "cell_type": "markdown",
      "source": [
        "## Set the weight for each criterion\n",
        "> Please note that the size and order of the weights array must match the number of criteria defined in the `CRITERIA` dictionary."
      ],
      "id": "a91fa6d97f38eef9"
    },
    {
      "metadata": {
        "id": "6486d5901d5ebd13"
      },
      "cell_type": "code",
      "source": [
        "# Here's your criteria keys for reference\n",
        "for key in criteria_keys:\n",
        "    print(key)"
      ],
      "id": "6486d5901d5ebd13",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "578bb9da68f76611"
      },
      "cell_type": "code",
      "source": [
        "# Set the weight for each criterion\n",
        "\n",
        "weights = [1, 1, 1, 1, 1, 1] # Change me\n",
        "\n",
        "weighted_votes = [ deepcopy(vote) for vote in votes ]\n",
        "\n",
        "for weighted_vote, vote in zip(weighted_votes, votes):\n",
        "    for i, key in enumerate(criteria_keys):\n",
        "        weighted_vote['df'][key] = vote['df'][key] * weights[i]\n",
        "    weighted_vote['df']['score_sum'] = weighted_vote['df'][criteria_keys].sum(axis=1)\n",
        "\n",
        "# Display weighted scores\n",
        "for weighted_vote in weighted_votes:\n",
        "    print(f\"Weighted scores for persona: {weighted_vote['persona']}\")\n",
        "    display(weighted_vote['df'])"
      ],
      "id": "578bb9da68f76611",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "44b5ccd26d0ea43c"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate the final scores"
      ],
      "id": "44b5ccd26d0ea43c"
    },
    {
      "metadata": {
        "id": "1198f41ed83fce2"
      },
      "cell_type": "code",
      "source": [
        "final_scores = weighted_votes[0]['df'].copy().drop(columns=['score_sum'])\n",
        "for vote in weighted_votes[1:]:\n",
        "    final_scores[criteria_keys] += vote['df'][criteria_keys]\n",
        "\n",
        "final_scores = final_scores.drop(columns=['Rank'])\n",
        "final_scores['total score'] = final_scores[criteria_keys].sum(axis=1)\n",
        "# Sort by total score\n",
        "final_scores = final_scores.sort_values(by='total score')\n",
        "final_scores = final_scores.drop(columns=['total score'])\n",
        "\n",
        "final_scores = final_scores.set_index(EVALUATION_SUBJECT)\n",
        "\n",
        "final_scores"
      ],
      "id": "1198f41ed83fce2",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a7bdb1319cf9b5ba"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "id": "a7bdb1319cf9b5ba"
    },
    {
      "metadata": {
        "id": "a76260b57e50f2d0"
      },
      "cell_type": "markdown",
      "source": [
        "## Score Breakdown\n",
        "This shows the breakdown of scores for each band across all criteria."
      ],
      "id": "a76260b57e50f2d0"
    },
    {
      "metadata": {
        "id": "3bbf48c8abd4e957"
      },
      "cell_type": "code",
      "source": [
        "final_scores.plot(kind='barh', stacked=True, figsize=(12, 7), colormap='tab20c')\n",
        "plt.title(\"Score Breakdown\")\n",
        "plt.xlabel(\"Total Score\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "id": "3bbf48c8abd4e957",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ce9af0c9e236ac55"
      },
      "cell_type": "markdown",
      "source": [
        "## Radar Chart"
      ],
      "id": "ce9af0c9e236ac55"
    },
    {
      "metadata": {
        "id": "35c5e5175409787f"
      },
      "cell_type": "code",
      "source": [
        "items = final_scores.mean(axis=1).sort_values(ascending=False).index\n",
        "\n",
        "for item in items:\n",
        "    values = final_scores.loc[item, criteria_keys].values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "\n",
        "    angles = np.linspace(0, 2 * np.pi, len(criteria_keys), endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    ax.plot(angles, values, linewidth=2, label=item)\n",
        "    ax.fill(angles, values, alpha=0.3)\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), criteria_keys)\n",
        "    ax.set_title(f\"{item} Score Profile\")\n",
        "    plt.show()"
      ],
      "id": "35c5e5175409787f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c9b25b82c4acb69c"
      },
      "cell_type": "markdown",
      "source": [
        "## Heatmap of Criteria Correlation"
      ],
      "id": "c9b25b82c4acb69c"
    },
    {
      "metadata": {
        "id": "292984ab92d9678d"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(final_scores[criteria_keys].corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Between Evaluation Criteria\")\n",
        "plt.show()"
      ],
      "id": "292984ab92d9678d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6041037d5148b6cf"
      },
      "cell_type": "markdown",
      "source": [
        "## Heatmap of individual rankings"
      ],
      "id": "6041037d5148b6cf"
    },
    {
      "cell_type": "code",
      "id": "90636900110fb752",
      "metadata": {
        "id": "90636900110fb752"
      },
      "source": [
        "rank_matrix = []\n",
        "band_names = weighted_votes[0]['df'][EVALUATION_SUBJECT].tolist()\n",
        "for weighted_vote in weighted_votes:\n",
        "    rank_row = dict(zip(weighted_vote['df'][EVALUATION_SUBJECT], list(weighted_vote['df']['Rank'])))\n",
        "    rank_matrix.append(rank_row)\n",
        "\n",
        "rank_df = pd.DataFrame(rank_matrix, index=[i['persona'] for i in weighted_votes], columns=band_names)\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.heatmap(rank_df, cmap=\"coolwarm\", annot=True, fmt=\"d\", cbar_kws={\"label\": \"Rank (lower is better)\"})\n",
        "plt.title(f\"{EVALUATION_SUBJECT} Rankings by Persona\")\n",
        "plt.xlabel(EVALUATION_SUBJECT)\n",
        "plt.ylabel(\"Persona\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f730b3a7fafa00f8"
      },
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "id": "f730b3a7fafa00f8"
    },
    {
      "metadata": {
        "id": "c7ac6fb6d00836fc"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary on rankings"
      ],
      "id": "c7ac6fb6d00836fc"
    },
    {
      "cell_type": "code",
      "id": "3e8f73779fc72a1f",
      "metadata": {
        "id": "3e8f73779fc72a1f"
      },
      "source": [
        "item_ranks = {item: [] for item in ITEMS}\n",
        "for vote in weighted_votes:\n",
        "    for item, rank in zip(vote['df'][EVALUATION_SUBJECT], vote['df']['Rank']):\n",
        "        item_ranks[item].append(rank)\n",
        "\n",
        "stats = {\n",
        "    item: {\n",
        "        \"mean_rank\": np.mean(ranks),\n",
        "        \"std_dev\": np.std(ranks),\n",
        "        \"min_rank\": min(ranks),\n",
        "        \"max_rank\": max(ranks)\n",
        "    }\n",
        "    for item, ranks in item_ranks.items()\n",
        "}\n",
        "\n",
        "# change this value to sort by a different statistic ⬇️. This currently shows the diversity of opinions.\n",
        "rank_stats_df = pd.DataFrame(stats).T.sort_values(\"std_dev\", ascending=False)\n",
        "\n",
        "rank_stats_df\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64237fa96d3328ba"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary on scores"
      ],
      "id": "64237fa96d3328ba"
    },
    {
      "metadata": {
        "id": "80f9b0d5af1d2c00"
      },
      "cell_type": "code",
      "source": [
        "item_scores = {f'{item}: {criteria}': [] for item in ITEMS for criteria in criteria_keys}\n",
        "for vote in weighted_votes:\n",
        "    for criteria in criteria_keys:\n",
        "        for item, score in zip(vote['df'][EVALUATION_SUBJECT], vote['df'][criteria]):\n",
        "            item_scores[f'{item}: {criteria}'].append(score)\n",
        "\n",
        "stats = {\n",
        "    item: {\n",
        "        \"mean_score\": np.mean(ranks),\n",
        "        \"std_dev\": np.std(ranks),\n",
        "        \"min_score\": min(ranks),\n",
        "        \"max_score\": max(ranks)\n",
        "    }\n",
        "    for item, ranks in item_scores.items()\n",
        "}\n",
        "\n",
        "# change this value to sort by a different statistic ⬇️. This currently shows the diversity of opinions.\n",
        "score_stats_df = pd.DataFrame(stats).T.sort_values(\"std_dev\", ascending=False)\n",
        "\n",
        "score_stats_df"
      ],
      "id": "80f9b0d5af1d2c00",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}